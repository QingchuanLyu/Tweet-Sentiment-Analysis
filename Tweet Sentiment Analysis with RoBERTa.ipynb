{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Sentiment Analysis with RoBERTa\n",
    "\n",
    "Goal: predict which part of a tweet could imply its sentiment.\n",
    "\n",
    "### Acknowledgements\n",
    "\n",
    "RoBERTa Glossary: https://huggingface.co/transformers/model_doc/roberta.html\n",
    "\n",
    "I learned a lot by reading the following kernels on Kaggle:\n",
    "\n",
    "* https://www.kaggle.com/vbmokin/tse2020-roberta-cnn-outlier-analysis-3chr/data\n",
    "* https://www.kaggle.com/vbmokin/nlp-eda-bag-of-words-tf-idf-glove-bert\n",
    "* https://www.kaggle.com/vbmokin/covid-19-week5-global-forecasting-eda-extratr\n",
    "* https://www.kaggle.com/khoongweihao/tse2020-roberta-cnn-random-seed-distribution\n",
    "* https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/142404#809872\n",
    "* https://www.kaggle.com/seesee/faster-2x-tf-roberta\n",
    "* https://www.kaggle.com/cdeotte/tf-roberta\n",
    "* https://www.kaggle.com/abhishek/roberta-inference-5-folds\n",
    "* https://www.kaggle.com/khoongweihao/tse2020-roberta-cnn-random-seed-distribution?scriptVersionId=34448972"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add-ons\n",
    "* Customized stopwords\n",
    "* Lemmantizer\n",
    "* Remove periods/comma, but keep asterisks\n",
    "* Stemming\n",
    "* Tuning and regularization\n",
    "* Different learning rates for different epochs\n",
    "\n",
    "*Note*: Stopwords, lemmantizer, removal of punctuations and stemming were not implemented in this project, as the goal is to predict which part of a text is a valid indication of sentiment. I added these methods here just to demonstrate data cleaning techniques in NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns; sns.set(style='white')\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from transformers import *\n",
    "import tokenizers\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import nltk\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "#not hiding long texts\n",
    "pd.set_option('max_colwidth', 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input trainning data\n",
    "train = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv').fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#customize STOPWORDS\n",
    "STOPWORDS -= {\"mustn't\",\"would\", \"but\", \"only\", \"too\", \"over\", \"with\", \"down\", \"against\", \"won't\", \"haven't\", \"below\", \"like\", \"all\", \"can't\", \"not\", \"isn't\", \"wouldn't\", 'off', \"doesn't\", 'ought', \"aren't\",\"didn't\", \"don't\", 'no', \"couldn't\", 'cannot','what',\"wasn't\", \"weren't\",'above','nor', \"shan't\", \"however\", \"hadn't\",  \"up\", \"why\"}\n",
    "def remove_stopword(x):\n",
    "    return [y for y in x.split() if y not in STOPWORDS]\n",
    "\n",
    "#apply customized STOPWORDS\n",
    "train.text = train.text.astype(str)\n",
    "train.selected_text = train.selected_text.astype(str)\n",
    "train['selected_text'] = train['selected_text'].apply(lambda x: \" \".join(remove_stopword(x)))\n",
    "train['text'] = train['text'].apply(lambda x: \" \".join(remove_stopword(x)))\n",
    "\n",
    "#remove the noise of empty texts\n",
    "train = train[(train.text != '') & (train.selected_text != '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming\n",
    "train['selected_text'] = train['selected_text'].apply(lambda x: \" \".join([ps.stem(word) for word in x.split()]))\n",
    "train['text'] = train['text'].apply(lambda x: \" \".join([ps.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmantizer\n",
    "def lemmatize_text(x):\n",
    "    return [lemmatizer.lemmatize(w, 'v') for w in x.split()]\n",
    "train['selected_text'] = train['selected_text'].apply(lambda x: \" \".join(lemmatize_text(x)))\n",
    "train['text'] = train['text'].apply(lambda x: \" \".join(lemmatize_text(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove part of punctuations; cannot remove all of them with regular expressions as some texts only contain punctuations\n",
    "def remove_periods(x):\n",
    "    return [word.translate(str.maketrans({',': '', '.': '', '|': ''})) for word in x.split()]\n",
    "train['selected_text'] = train['selected_text'].apply(lambda x: \" \".join(remove_periods(x)))\n",
    "train['text'] = train['text'].apply(lambda x: \" \".join(remove_periods(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#don't need the column of text ids\n",
    "train=train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenice training data for RoBERTa to understand\n",
    "ct = train.shape[0]\n",
    "input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(train.shape[0]):\n",
    "    \n",
    "    # FIND OVERLAP\n",
    "    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "    text2 = \" \".join(train.loc[k,'selected_text'].split())\n",
    "    idx = text1.find(text2)\n",
    "    chars = np.zeros((len(text1)))\n",
    "    chars[idx:idx+len(text2)]=1\n",
    "    if text1[idx-1]==' ': chars[idx-1] = 1 \n",
    "    enc = tokenizer.encode(text1)  \n",
    "        \n",
    "    # ID_OFFSETS\n",
    "    offsets = []; idx=0\n",
    "    for t in enc.ids:\n",
    "        w = tokenizer.decode([t])\n",
    "        offsets.append((idx,idx+len(w)))\n",
    "        idx += len(w)\n",
    "    \n",
    "    # START END TOKENS\n",
    "    toks = []\n",
    "    for i,(a,b) in enumerate(offsets):\n",
    "        sm = np.sum(chars[a:b])\n",
    "        if sm>0: toks.append(i) \n",
    "        \n",
    "    s_tok = sentiment_id[train.loc[k,'sentiment']]\n",
    "    input_ids[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n",
    "    attention_mask[k,:len(enc.ids)+3] = 1\n",
    "    if len(toks)>0:\n",
    "        start_tokens[k,toks[0]+2] = 1\n",
    "        end_tokens[k,toks[-1]+2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "#input test data\n",
    "test = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv').fillna('')\n",
    "\n",
    "#tokenize test and training data in the same way\n",
    "ct = test.shape[0]\n",
    "input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(test.shape[0]):\n",
    "        \n",
    "    # INPUT_IDS\n",
    "    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "    enc = tokenizer.encode(text1)                \n",
    "    s_tok = sentiment_id[test.loc[k,'sentiment']]\n",
    "    input_ids_t[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n",
    "    attention_mask_t[k,:len(enc.ids)+3] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Hyperparameters <a class=\"anchor\" id=\"3.1\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tune hyperparameters; learning rates were tuned inside of model along with each epoch\n",
    "MAX_LEN = 106\n",
    "EPOCHS = 7 \n",
    "BATCH_SIZE = 32 \n",
    "PAD_ID = 1\n",
    "#set up seeds\n",
    "SEED = 88888\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "#to address overfitting and overconfidence\n",
    "LABEL_SMOOTHING = 0.1\n",
    "Dropout_new = 0.18     \n",
    "#five folds\n",
    "n_split = 5            \n",
    "\n",
    "#specify the last sorting number of sentiments\n",
    "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model <a class=\"anchor\" id=\"3.2\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-trained model path\n",
    "PATH = '/kaggle/input/tf-roberta/'\n",
    "#RoBERTa requires tokenizer\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file=PATH+'vocab-roberta-base.json', \n",
    "    merges_file=PATH+'merges-roberta-base.txt', \n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "#save the weight of a layer: h=WX+b, where W is the weight matrix\n",
    "def save_weights(model, dst_fn):\n",
    "    weights = model.get_weights()\n",
    "    with open(dst_fn, 'wb') as f:\n",
    "        pickle.dump(weights, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the weights by the saved weights\n",
    "def load_weights(model, weight_fn):\n",
    "    with open(weight_fn, 'rb') as f:\n",
    "        weights = pickle.load(f)\n",
    "    model.set_weights(weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define logistic loss function\n",
    "def loss_fn(y_true, y_pred):\n",
    "    #select part of true labels corresponding to predictions\n",
    "    ll = tf.shape(y_pred)[1]\n",
    "    y_true = y_true[:, :ll]\n",
    "    #log loss with label smoothing\n",
    "    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred,\n",
    "        from_logits=False, label_smoothing=LABEL_SMOOTHING)\n",
    "    #compute the mean of all dimensions of a tensor: optimize the overall loss across all samples\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a networking model with Tensorflow\n",
    "def build_model():\n",
    "    #with functional API, create model's inputs and outputs\n",
    "    #####inputs#####\n",
    "    #initiate batch size and dtype\n",
    "    #input ids: numerical representation of tokens building the sequences that will be used as model input\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    #attention mask: for the BertTokenizer, 1 indicaes attended indices, and 0 indicates padding indices\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    #token_type_ids: binary indicators to separate sentences, especially questions (1) and answers (0)\n",
    "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    \n",
    "    #To put texts with different length in a tensor, I pad short texts then reduce to the max length\n",
    "    padding = tf.cast(tf.equal(ids, PAD_ID), tf.int32)\n",
    "    #max length - reduced sum across rows of padding\n",
    "    lens = MAX_LEN - tf.reduce_sum(padding, -1)\n",
    "    #keep the max across all dimensions\n",
    "    max_len = tf.reduce_max(lens)\n",
    "    #pick the same length and put in a tensor\n",
    "    ids_ = ids[:, :max_len]\n",
    "    att_ = att[:, :max_len]\n",
    "    tok_ = tok[:, :max_len]\n",
    "    \n",
    "    #initialize configuration and then a model from the configuration\n",
    "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
    "    #import a pretrained model from Kaggle\n",
    "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
    "    #input hyperparameters\n",
    "    x = bert_model(ids_,attention_mask=att_,token_type_ids=tok_)\n",
    "    \n",
    "    #####output#####\n",
    "    #chain layer calls and specify model's forward pass\n",
    "    #dropout (overfitting): randomly select input to set to 0 at rate = 0.18\n",
    "    x1 = tf.keras.layers.Dropout(Dropout_new)(x[0])\n",
    "    #apply zero padding to convolution layer\n",
    "    x1 = tf.keras.layers.Conv1D(filters=768, kernel_size=2,padding='same')(x1)\n",
    "    #apply leaky versoin of ReLU and generate output with the same shape\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n",
    "    #output (1 dim space) = dot(input, weights matrix)\n",
    "    x1 = tf.keras.layers.Dense(1)(x1)\n",
    "    #add an extra channel dimension and output shapes are (batch, 1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    #apply softmax activation function\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "    \n",
    "    #same as x1\n",
    "    x2 = tf.keras.layers.Dropout(Dropout_new)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n",
    "    x2 = tf.keras.layers.Dense(1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "    \n",
    "    #collect info of model together\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
    "    #use Adam algorithm as optimizer. tried others, such as Adamax based on infinity norm--didn't work well\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr) \n",
    "    #config logistic loss function and adam optimizer\n",
    "    model.compile(loss=loss_fn, optimizer=optimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "#compute Jaccard Index (similarity scores)\n",
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    if (len(a)==0) & (len(b)==0): return 0.5\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize starting and end points of prediction windows\n",
    "preds_start = np.zeros((input_ids_t.shape[0], MAX_LEN))\n",
    "preds_end = np.zeros((input_ids_t.shape[0], MAX_LEN))\n",
    "\n",
    "#1==>print out\n",
    "DISPLAY=1\n",
    "for i in range(5):\n",
    "    #mark each fold (4)\n",
    "    print('#'*25)\n",
    "    print('### MODEL %i'%(i+1))\n",
    "    print('#'*25)\n",
    "    \n",
    "    #load weights from pretrained model\n",
    "    K.clear_session()\n",
    "    model = build_model()\n",
    "    model.load_weights('/kaggle/input/model4/v4-roberta-%i.h5'%i)\n",
    "\n",
    "    print('Predicting Test...')\n",
    "    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
    "    #move forwards prediction windows\n",
    "    preds_start += preds[0]/n_splits\n",
    "    preds_end += preds[1]/n_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Prediction <a class=\"anchor\" id=\"4\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = []\n",
    "#output text one by one\n",
    "for k in range(input_ids_t.shape[0]):\n",
    "    #return the index of the maximum value\n",
    "    a = np.argmax(preds_start[k,])\n",
    "    b = np.argmax(preds_end[k,])\n",
    "    #if start>end, otuput text directly\n",
    "    if a>b: \n",
    "        st = test.loc[k,'text']\n",
    "    #if start<=end:\n",
    "    else:\n",
    "        #use \"+\" as a separator to join text splitted by space\n",
    "        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "        #tokenize \n",
    "        enc = tokenizer.encode(text1)\n",
    "        #decode one by one\n",
    "        st = tokenizer.decode(enc.ids[a-1:b])\n",
    "    all.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output 10 sample rows\n",
    "test['selected_text'] = all\n",
    "test[['textID','selected_text']].to_csv('sample.csv',index=False)\n",
    "test.sample(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
