{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "* https://www.kaggle.com/vbmokin/tse2020-roberta-cnn-outlier-analysis-3chr/data\n",
    "* [NLP - EDA, Bag of Words, TF IDF, GloVe, BERT](https://www.kaggle.com/vbmokin/nlp-eda-bag-of-words-tf-idf-glove-bert)\n",
    "* [COVID-19 (Week5) Global Forecasting - EDA&ExtraTR](https://www.kaggle.com/vbmokin/covid-19-week5-global-forecasting-eda-extratr)\n",
    "* [TSE2020] RoBERTa (CNN) & Random Seed Distribution (https://www.kaggle.com/khoongweihao/tse2020-roberta-cnn-random-seed-distribution)\n",
    "* Chris Deotte's post: https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/142404#809872\n",
    "* [Faster (2x) TF roBERTa](https://www.kaggle.com/seesee/faster-2x-tf-roberta)\n",
    "* Many thanks to Chris Deotte for his TF roBERTa dataset at https://www.kaggle.com/cdeotte/tf-roberta\n",
    "* https://www.kaggle.com/abhishek/roberta-inference-5-folds\n",
    "* Code reference: https://www.kaggle.com/khoongweihao/tse2020-roberta-cnn-random-seed-distribution?scriptVersionId=34448972"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Add-ons\n",
    "* Customized stopwords\n",
    "* Lemmantizer\n",
    "* Remove periods/comma, but keep asterisks\n",
    "* Stemming\n",
    "* Tuning and regularization\n",
    "* Different learning rates for different epochs\n",
    "\n",
    "*Note*: Stopwords, lemmantizer, removal of punctuations and stemming were not used, due to the nature of this particular project. I added them here just to demonstrate data cleaning techniques in NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly.express as px\n",
    "import seaborn as sns; sns.set(style='white')\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from transformers import *\n",
    "import tokenizers\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import nltk\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "#STOPWORDS -= {\"mustn't\",\"would\", \"but\", \"only\", \"too\", \"over\", \"with\", \"down\", \"against\", \"won't\", \"haven't\", \"below\", \"like\", \"all\", \"can't\", \"not\", \"isn't\", \"wouldn't\", 'off', \"doesn't\", 'ought', \"aren't\",\"didn't\", \"don't\", 'no', \"couldn't\", 'cannot','what',\"wasn't\", \"weren't\",'above','nor', \"shan't\", \"however\", \"hadn't\",  \"up\", \"why\"}\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "pd.set_option('max_colwidth', 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv').fillna('')\n",
    "#train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword(x):\n",
    "    return [y for y in x.split() if y not in STOPWORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.text = train.text.astype(str)\n",
    "train.selected_text = train.selected_text.astype(str)\n",
    "\n",
    "train['selected_text'] = train['selected_text'].apply(lambda x: \" \".join(remove_stopword(x)))\n",
    "train['text'] = train['text'].apply(lambda x: \" \".join(remove_stopword(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the noise of empty texts\n",
    "train = train[(train.text != '') & (train.selected_text != '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming\n",
    "train['selected_text'] = train['selected_text'].apply(lambda x: \" \".join([ps.stem(word) for word in x.split()]))\n",
    "train['text'] = train['text'].apply(lambda x: \" \".join([ps.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmantizer\n",
    "def lemmatize_text(x):\n",
    "    return [lemmatizer.lemmatize(w, 'v') for w in x.split()]\n",
    "train['selected_text'] = train['selected_text'].apply(lambda x: \" \".join(lemmatize_text(x)))\n",
    "train['text'] = train['text'].apply(lambda x: \" \".join(lemmatize_text(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_periods(x):\n",
    "    return [word.translate(str.maketrans({',': '', '.': '', '|': ''})) for word in x.split()]\n",
    "train['selected_text'] = train['selected_text'].apply(lambda x: \" \".join(remove_periods(x)))\n",
    "train['text'] = train['text'].apply(lambda x: \" \".join(remove_periods(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv').fillna('')\n",
    "\n",
    "ct = test.shape[0]\n",
    "input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(test.shape[0]):\n",
    "        \n",
    "    # INPUT_IDS\n",
    "    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "    enc = tokenizer.encode(text1)                \n",
    "    s_tok = sentiment_id[test.loc[k,'sentiment']]\n",
    "    input_ids_t[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n",
    "    attention_mask_t[k,:len(enc.ids)+3] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters <a class=\"anchor\" id=\"3.1\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 106\n",
    "PATH = '../input/tf-roberta/'\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file=PATH+'vocab-roberta-base.json', \n",
    "    merges_file=PATH+'merges-roberta-base.txt', \n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "EPOCHS = 7 # originally 3\n",
    "BATCH_SIZE = 32 # originally 32\n",
    "PAD_ID = 1\n",
    "SEED = 88888\n",
    "LABEL_SMOOTHING = 0.1\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n",
    "\n",
    "Dropout_new = 0.18     # originally 0.1\n",
    "n_split = 4            # originally 5\n",
    "#lr = 2e-4              # originally 3e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model <a class=\"anchor\" id=\"3.2\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def save_weights(model, dst_fn):\n",
    "    weights = model.get_weights()\n",
    "    with open(dst_fn, 'wb') as f:\n",
    "        pickle.dump(weights, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_weights(model, weight_fn):\n",
    "    with open(weight_fn, 'rb') as f:\n",
    "        weights = pickle.load(f)\n",
    "    model.set_weights(weights)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y_true, y_pred):\n",
    "    # adjust the targets for sequence bucketing\n",
    "    ll = tf.shape(y_pred)[1]\n",
    "    y_true = y_true[:, :ll]\n",
    "    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred,\n",
    "        from_logits=False, label_smoothing=LABEL_SMOOTHING)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    padding = tf.cast(tf.equal(ids, PAD_ID), tf.int32)\n",
    "\n",
    "    lens = MAX_LEN - tf.reduce_sum(padding, -1)\n",
    "    max_len = tf.reduce_max(lens)\n",
    "    ids_ = ids[:, :max_len]\n",
    "    att_ = att[:, :max_len]\n",
    "    tok_ = tok[:, :max_len]\n",
    "\n",
    "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
    "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
    "    x = bert_model(ids_,attention_mask=att_,token_type_ids=tok_)\n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(Dropout_new)(x[0])\n",
    "    x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n",
    "    x1 = tf.keras.layers.Dense(1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(Dropout_new)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n",
    "    x2 = tf.keras.layers.Dense(1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr) \n",
    "    model.compile(loss=loss_fn, optimizer=optimizer)\n",
    "    \n",
    "    # this is required as `model.predict` needs a fixed size!\n",
    "    x1_padded = tf.pad(x1, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n",
    "    x2_padded = tf.pad(x2, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n",
    "    \n",
    "    padded_model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1_padded,x2_padded])\n",
    "    return model, padded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    if (len(a)==0) & (len(b)==0): return 0.5\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "ct = train.shape[0]\n",
    "input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(train.shape[0]):\n",
    "    \n",
    "    # FIND OVERLAP\n",
    "    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "    text2 = \" \".join(train.loc[k,'selected_text'].split())\n",
    "    idx = text1.find(text2)\n",
    "    chars = np.zeros((len(text1)))\n",
    "    chars[idx:idx+len(text2)]=1\n",
    "    if text1[idx-1]==' ': chars[idx-1] = 1 \n",
    "    enc = tokenizer.encode(text1)  \n",
    "        \n",
    "    # ID_OFFSETS\n",
    "    offsets = []; idx=0\n",
    "    for t in enc.ids:\n",
    "        w = tokenizer.decode([t])\n",
    "        offsets.append((idx,idx+len(w)))\n",
    "        idx += len(w)\n",
    "    \n",
    "    # START END TOKENS\n",
    "    toks = []\n",
    "    for i,(a,b) in enumerate(offsets):\n",
    "        sm = np.sum(chars[a:b])\n",
    "        if sm>0: toks.append(i) \n",
    "        \n",
    "    s_tok = sentiment_id[train.loc[k,'sentiment']]\n",
    "    input_ids[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n",
    "    attention_mask[k,:len(enc.ids)+3] = 1\n",
    "    if len(toks)>0:\n",
    "        start_tokens[k,toks[0]+2] = 1\n",
    "        end_tokens[k,toks[-1]+2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### FOLD 1\n",
      "#########################\n",
      "Train on 20610 samples, validate on 6871 samples\n",
      "20610/20610 [==============================] - 137s 7ms/sample - loss: 7.6500 - activation_loss: 3.7844 - activation_1_loss: 3.8637 - val_loss: 5.8122 - val_activation_loss: 2.8834 - val_activation_1_loss: 2.9329\n",
      "Train on 20610 samples, validate on 6871 samples\n",
      "Epoch 2/2\n",
      "20610/20610 [==============================] - 129s 6ms/sample - loss: 7.8620 - activation_2_loss: 4.0151 - activation_3_loss: 3.8443 - val_loss: 5.7587 - val_activation_2_loss: 2.8866 - val_activation_3_loss: 2.8763\n",
      "Train on 20610 samples, validate on 6871 samples\n",
      "Epoch 3/3\n",
      "20610/20610 [==============================] - 132s 6ms/sample - loss: 7.4044 - activation_4_loss: 3.7669 - activation_5_loss: 3.6367 - val_loss: 5.5995 - val_activation_4_loss: 2.8897 - val_activation_5_loss: 2.7142\n",
      "Train on 20610 samples, validate on 6871 samples\n",
      "Epoch 4/4\n",
      "20610/20610 [==============================] - 126s 6ms/sample - loss: 2.9746 - activation_6_loss: 1.4962 - activation_7_loss: 1.4770 - val_loss: 2.5448 - val_activation_6_loss: 1.2929 - val_activation_7_loss: 1.2536\n",
      "Train on 20610 samples, validate on 6871 samples\n",
      "Epoch 5/5\n",
      "20610/20610 [==============================] - 125s 6ms/sample - loss: 2.9647 - activation_8_loss: 1.4984 - activation_9_loss: 1.4693 - val_loss: 2.6005 - val_activation_8_loss: 1.3033 - val_activation_9_loss: 1.2990\n",
      "Train on 20610 samples, validate on 6871 samples\n",
      "Epoch 6/6\n",
      "20610/20610 [==============================] - 131s 6ms/sample - loss: 3.0067 - activation_10_loss: 1.5155 - activation_11_loss: 1.4951 - val_loss: 2.5628 - val_activation_10_loss: 1.3006 - val_activation_11_loss: 1.2637\n",
      "Train on 20610 samples, validate on 6871 samples\n",
      "Epoch 7/7\n",
      "20610/20610 [==============================] - 128s 6ms/sample - loss: 2.9720 - activation_12_loss: 1.4911 - activation_13_loss: 1.4797 - val_loss: 2.5360 - val_activation_12_loss: 1.2874 - val_activation_13_loss: 1.2503\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "6871/6871 [==============================] - 20s 3ms/sample\n",
      "Predicting all Train for Outlier analysis...\n",
      "27481/27481 [==============================] - 66s 2ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 8s 2ms/sample\n",
      ">>>> FOLD 1 Jaccard = 0.7105454328375299\n",
      "\n",
      "#########################\n",
      "### FOLD 2\n",
      "#########################\n",
      "Train on 20611 samples, validate on 6870 samples\n",
      "20611/20611 [==============================] - 134s 6ms/sample - loss: 8.2056 - activation_loss: 4.1703 - activation_1_loss: 4.0312 - val_loss: 5.8125 - val_activation_loss: 2.8670 - val_activation_1_loss: 2.9488\n",
      "Train on 20611 samples, validate on 6870 samples\n",
      "Epoch 2/2\n",
      "20611/20611 [==============================] - 130s 6ms/sample - loss: 8.3809 - activation_2_loss: 4.3010 - activation_3_loss: 4.0756 - val_loss: 5.6932 - val_activation_2_loss: 2.8642 - val_activation_3_loss: 2.8325\n",
      "Train on 20611 samples, validate on 6870 samples\n",
      "Epoch 3/3\n",
      "20611/20611 [==============================] - 129s 6ms/sample - loss: 7.2758 - activation_4_loss: 3.4937 - activation_5_loss: 3.7784 - val_loss: 5.5822 - val_activation_4_loss: 2.8634 - val_activation_5_loss: 2.7224\n",
      "Train on 20611 samples, validate on 6870 samples\n",
      "Epoch 4/4\n",
      "20611/20611 [==============================] - 126s 6ms/sample - loss: 2.9904 - activation_6_loss: 1.5039 - activation_7_loss: 1.4840 - val_loss: 2.5514 - val_activation_6_loss: 1.2978 - val_activation_7_loss: 1.2539\n",
      "Train on 20611 samples, validate on 6870 samples\n",
      "Epoch 5/5\n",
      "20611/20611 [==============================] - 125s 6ms/sample - loss: 2.9699 - activation_8_loss: 1.4917 - activation_9_loss: 1.4762 - val_loss: 2.5756 - val_activation_8_loss: 1.3082 - val_activation_9_loss: 1.2678\n",
      "Train on 20611 samples, validate on 6870 samples\n",
      "Epoch 6/6\n",
      "20611/20611 [==============================] - 133s 6ms/sample - loss: 3.0183 - activation_10_loss: 1.5175 - activation_11_loss: 1.4994 - val_loss: 2.5624 - val_activation_10_loss: 1.3049 - val_activation_11_loss: 1.2581\n",
      "Train on 20611 samples, validate on 6870 samples\n",
      "Epoch 7/7\n",
      "20611/20611 [==============================] - 122s 6ms/sample - loss: 2.9518 - activation_12_loss: 1.4776 - activation_13_loss: 1.4723 - val_loss: 2.5662 - val_activation_12_loss: 1.2903 - val_activation_13_loss: 1.2762\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "6870/6870 [==============================] - 19s 3ms/sample\n",
      "Predicting all Train for Outlier analysis...\n",
      "27481/27481 [==============================] - 66s 2ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 9s 2ms/sample\n",
      ">>>> FOLD 2 Jaccard = 0.7013206028928282\n",
      "\n",
      "#########################\n",
      "### FOLD 3\n",
      "#########################\n",
      "Train on 20611 samples, validate on 6870 samples\n",
      "20611/20611 [==============================] - 129s 6ms/sample - loss: 7.0875 - activation_loss: 3.2976 - activation_1_loss: 3.7876 - val_loss: 5.6803 - val_activation_loss: 2.8555 - val_activation_1_loss: 2.8290\n",
      "Train on 20611 samples, validate on 6870 samples\n",
      "Epoch 2/2\n",
      "20611/20611 [==============================] - 128s 6ms/sample - loss: 7.9441 - activation_2_loss: 3.8651 - activation_3_loss: 4.0763 - val_loss: 5.5589 - val_activation_2_loss: 2.8548 - val_activation_3_loss: 2.7084\n",
      "Train on 20611 samples, validate on 6870 samples\n",
      "Epoch 3/3\n",
      "20611/20611 [==============================] - 131s 6ms/sample - loss: 7.7822 - activation_4_loss: 3.9481 - activation_5_loss: 3.8302 - val_loss: 5.7640 - val_activation_4_loss: 2.8681 - val_activation_5_loss: 2.8994\n",
      "Train on 20611 samples, validate on 6870 samples\n",
      "Epoch 4/4\n",
      "20611/20611 [==============================] - 123s 6ms/sample - loss: 2.9301 - activation_12_loss: 1.4707 - activation_13_loss: 1.4580 - val_loss: 2.5461 - val_activation_12_loss: 1.2816 - val_activation_13_loss: 1.2663\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      " 864/6870 [==>...........................] - ETA: 37s"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
    "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "preds_start_train = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "preds_end_train = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_split,shuffle=True,random_state=SEED)\n",
    "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n",
    "\n",
    "    print('#'*25)\n",
    "    print('### FOLD %i'%(fold+1))\n",
    "    print('#'*25)\n",
    "    \n",
    "    K.clear_session()\n",
    "    \n",
    "        \n",
    "    #sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "    #    '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
    "    #    save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "    inpT = [input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]]\n",
    "    targetT = [start_tokens[idxT,], end_tokens[idxT,]]\n",
    "    inpV = [input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]]\n",
    "    targetV = [start_tokens[idxV,], end_tokens[idxV,]]\n",
    "    # sort the validation data\n",
    "    shuffleV = np.int32(sorted(range(len(inpV[0])), key=lambda k: (inpV[0][k] == PAD_ID).sum(), reverse=True))\n",
    "    inpV = [arr[shuffleV] for arr in inpV]\n",
    "    targetV = [arr[shuffleV] for arr in targetV]\n",
    "    weight_fn = '%s-roberta-%i.h5'%(VER,fold)\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        if epoch < 4:\n",
    "            lr = 0.008\n",
    "        elif epoch >= 4:\n",
    "            lr = 3e-5\n",
    "        model, padded_model = build_model()\n",
    "        # sort and shuffle: We add random numbers to not have the same order in each epoch\n",
    "        shuffleT = np.int32(sorted(range(len(inpT[0])), key=lambda k: (inpT[0][k] == PAD_ID).sum() + np.random.randint(-3, 3), reverse=True))\n",
    "        # shuffle in batches, otherwise short batches will always come in the beginning of each epoch\n",
    "        num_batches = math.ceil(len(shuffleT) / BATCH_SIZE)\n",
    "        batch_inds = np.random.permutation(num_batches)\n",
    "        shuffleT_ = []\n",
    "        for batch_ind in batch_inds:\n",
    "            shuffleT_.append(shuffleT[batch_ind * BATCH_SIZE: (batch_ind + 1) * BATCH_SIZE])\n",
    "        shuffleT = np.concatenate(shuffleT_)\n",
    "        # reorder the input data\n",
    "        inpT = [arr[shuffleT] for arr in inpT]\n",
    "        targetT = [arr[shuffleT] for arr in targetT]\n",
    "        model.fit(inpT, targetT, \n",
    "            epochs=epoch, initial_epoch=epoch - 1, batch_size=BATCH_SIZE, verbose=DISPLAY, callbacks=[],\n",
    "            validation_data=(inpV, targetV), shuffle=False)  # don't shuffle in `fit`\n",
    "        save_weights(model, weight_fn)\n",
    "\n",
    "    print('Loading model...')\n",
    "    # model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
    "    load_weights(model, weight_fn)\n",
    "\n",
    "    print('Predicting OOF...')\n",
    "    oof_start[idxV,],oof_end[idxV,] = padded_model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
    "    \n",
    "    print('Predicting all Train for Outlier analysis...')\n",
    "    preds_train = padded_model.predict([input_ids,attention_mask,token_type_ids],verbose=DISPLAY)\n",
    "    preds_start_train += preds_train[0]/skf.n_splits\n",
    "    preds_end_train += preds_train[1]/skf.n_splits\n",
    "\n",
    "    print('Predicting Test...')\n",
    "    preds = padded_model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
    "    preds_start += preds[0]/skf.n_splits\n",
    "    preds_end += preds[1]/skf.n_splits\n",
    "    \n",
    "    # DISPLAY FOLD JACCARD\n",
    "    all = []\n",
    "    for k in idxV:\n",
    "        a = np.argmax(oof_start[k,])\n",
    "        b = np.argmax(oof_end[k,])\n",
    "        if a>b: \n",
    "            st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n",
    "        else:\n",
    "            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "            enc = tokenizer.encode(text1)\n",
    "            st = tokenizer.decode(enc.ids[a-2:b-1])\n",
    "        all.append(jaccard(st,train.loc[k,'selected_text']))\n",
    "    jac.append(np.mean(all))\n",
    "    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission <a class=\"anchor\" id=\"4\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "all = []\n",
    "for k in range(input_ids_t.shape[0]):\n",
    "    a = np.argmax(preds_start[k,])\n",
    "    b = np.argmax(preds_end[k,])\n",
    "    if a>b: \n",
    "        st = test.loc[k,'text']\n",
    "    else:\n",
    "        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "        enc = tokenizer.encode(text1)\n",
    "        st = tokenizer.decode(enc.ids[a-2:b-1])\n",
    "    all.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2520</th>\n",
       "      <td>c4df4b1e33</td>\n",
       "      <td>Now I`m all sad because I`ll probabl...</td>\n",
       "      <td>negative</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1104</th>\n",
       "      <td>026d3b6611</td>\n",
       "      <td>what`s up? what happened? DM me if ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>what`s up? what happened? dm me if ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572</th>\n",
       "      <td>d109999477</td>\n",
       "      <td>can you play I Wish by Jordan Knight</td>\n",
       "      <td>neutral</td>\n",
       "      <td>can you play i wish by jordan knight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3089</th>\n",
       "      <td>0c0d180b5c</td>\n",
       "      <td>Thanks. Appreciate it.</td>\n",
       "      <td>positive</td>\n",
       "      <td>thanks. appreciate it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1862</th>\n",
       "      <td>5de3c34293</td>\n",
       "      <td>i am the only arabic girl who`s onli...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i am the only arabic girl who`s onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2210</th>\n",
       "      <td>cad8db609a</td>\n",
       "      <td>Why am I having such a hard time fal...</td>\n",
       "      <td>negative</td>\n",
       "      <td>hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2290</th>\n",
       "      <td>c9a52dee1f</td>\n",
       "      <td>Guess I`m gonna try the nap thing ag...</td>\n",
       "      <td>negative</td>\n",
       "      <td>i`m sure 2day won`t be different</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2982</th>\n",
       "      <td>f589c8b60d</td>\n",
       "      <td>Yeah, I thought that was very littl...</td>\n",
       "      <td>negative</td>\n",
       "      <td>yeah, i thought that was very littl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2377</th>\n",
       "      <td>46ffb6bd0e</td>\n",
       "      <td>i think you should follow me. as i ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>stalk me please</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>ef1eb004ff</td>\n",
       "      <td>oh you are too kind</td>\n",
       "      <td>positive</td>\n",
       "      <td>oh you are too kind</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          textID                                     text sentiment  \\\n",
       "2520  c4df4b1e33  Now I`m all sad because I`ll probabl...  negative   \n",
       "1104  026d3b6611   what`s up? what happened? DM me if ...   neutral   \n",
       "1572  d109999477     can you play I Wish by Jordan Knight   neutral   \n",
       "3089  0c0d180b5c                   Thanks. Appreciate it.  positive   \n",
       "1862  5de3c34293  i am the only arabic girl who`s onli...   neutral   \n",
       "2210  cad8db609a  Why am I having such a hard time fal...  negative   \n",
       "2290  c9a52dee1f  Guess I`m gonna try the nap thing ag...  negative   \n",
       "2982  f589c8b60d   Yeah, I thought that was very littl...  negative   \n",
       "2377  46ffb6bd0e   i think you should follow me. as i ...  positive   \n",
       "628   ef1eb004ff                      oh you are too kind  positive   \n",
       "\n",
       "                                selected_text  \n",
       "2520                                      sad  \n",
       "1104   what`s up? what happened? dm me if ...  \n",
       "1572     can you play i wish by jordan knight  \n",
       "3089                   thanks. appreciate it.  \n",
       "1862   i am the only arabic girl who`s onl...  \n",
       "2210                                     hard  \n",
       "2290         i`m sure 2day won`t be different  \n",
       "2982   yeah, i thought that was very littl...  \n",
       "2377                          stalk me please  \n",
       "628                       oh you are too kind  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['selected_text'] = all\n",
    "test[['textID','selected_text']].to_csv('submission.csv',index=False)\n",
    "test.sample(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
